# ğŸ¬ betterSearch - Step-by-Step Demo Script
## Exact Actions, Timing, and Talking Points

---

## â±ï¸ **TOTAL TIME: 7 MINUTES**

---

## ğŸ“‹ **PRE-DEMO SETUP** (Do this 10 minutes before)

### **1. Start Servers**
```bash
# Terminal 1: Backend
cd backend
python server.py
# Wait for: "ğŸ§  Neural Bridge Starting..." on port 8001

# Terminal 2: Frontend
npm run dev
# Wait for: Browser opens on http://localhost:3000
```

### **2. Prepare Files**
- [ ] 2-3 PDF research papers (ready in Downloads)
- [ ] 1-2 images/diagrams (ready)
- [ ] (Optional) 1 video file

### **3. Verify**
- [ ] Login with test username (e.g., "demo")
- [ ] Settings â†’ Verify Gemini API key configured
- [ ] Test one simple query to confirm AI works

---

## ğŸ¬ **DEMO SCRIPT**

---

### **SECTION 1: OPENING** â±ï¸ 30 seconds

#### **SAY:**
> "Students and researchers today are drowning in information. You have lecture videos, research papers, PDFs, textbooksâ€”but no way to actually search and reason across them all simultaneously."

#### **DO:**
1. Show the empty interface
2. Point to the screen
3. Say the hook line

#### **SAY:**
> "We built betterSearch: a multimodal AI knowledge base that combines Gemini 3's native multimodality with Graphon AI's trillion-token knowledge graphs. Let me show you what this enables."

**KEY POINT**: Set up the problem â†’ solution narrative

---

### **SECTION 2: MULTIMODAL UPLOAD** â±ï¸ 1 minute

#### **SAY:**
> "First, let's build your knowledge graph. Watch thisâ€”we're uploading multiple file types simultaneously: PDFs, videos, images. This is powered by Graphon AI's cross-modal architecture."

#### **DO:**
1. Click the **paperclip icon** (ğŸ“) in the input area
2. Select 2-3 PDF files from your prepared folder
   - Use your mouse to select multiple files
3. **WAIT** - Show files appear as previews above input
4. Click the **INGEST button** (ğŸ§  brain icon with "INGEST" text)
5. **WAIT** - Toast notification shows "Uploading X files to Knowledge Graph..."
6. **WAIT** - Success message: "Knowledge Graph Ready! (X files processed)"

#### **SAY (while uploading):**
> "Graphon is building a relationship graph across all these modalities in real-time. It's indexing video timestamps, PDF pages, and extracting text from images."

#### **SAY (when done):**
> "Done. We just indexed hundreds of pages in 30 seconds. This knowledge graph has trillion-token capacityâ€”persistent memory that never runs out of space."

**KEY POINTS**:
- âœ… Emphasize: "This is the Graphon Track prize feature"
- âœ… Mention: Cross-modal indexing
- âœ… Highlight: Speed and scale

---

### **SECTION 3: NATIVE MULTIMODAL QUERY** â±ï¸ 1.5 minutes

#### **SAY:**
> "Now the magic. Gemini 3 was trained natively on text, images, audio, and video together. So when I ask a question, it reasons across ALL my sources simultaneouslyâ€”not just retrieving chunks."

#### **DO:**
1. Press **Cmd+K** (or **Ctrl+K**) to open Command Palette
2. Type: "knowledge" or scroll to find it
3. Click **"Toggle Knowledge Mode"** (should show brain icon)
4. Verify: Status shows "ACTIVE" in purple
5. Press **Escape** to close Command Palette
6. In the input field, type:
   > "Based on the documents I uploaded, explain the key concepts and show me relevant diagrams"
7. Press **Enter** or click **EXECUTE**
8. **WAIT** - Response streams in
9. **POINT OUT** citations as they appear:
   - ğŸ¥ Video timestamps: "Lecture 3, 12:34-15:20"
   - ğŸ“„ PDF pages: "Research Paper, Page 47"
   - ğŸ–¼ï¸ Images: "Diagram from Slide 12"

#### **SAY (while response streams):**
> "Notice how Gemini 3 reasons natively across video, PDF, and images simultaneouslyâ€”this is native multimodality, not RAG stitching."

#### **SAY (when citations appear):**
> "Look at these exact citations: video timestamps, PDF page numbers, image references. This is cross-modal search in action. Every answer is grounded in your actual sources."

**KEY POINTS**:
- âœ… Native multimodality (emphasize this phrase)
- âœ… Exact citations with timestamps
- âœ… Cross-modal reasoning

---

### **SECTION 4: LONG-CONTEXT DEMONSTRATION** â±ï¸ 1 minute

#### **SAY:**
> "Gemini 3 has 1M token contextâ€”that's ~700K words. We're putting that to work. Let me show you what happens when you've ingested your entire course."

#### **DO:**
1. Ask a question that requires reasoning across multiple documents:
   > "Compare and contrast the approaches discussed in all the papers I uploaded. What are the key differences?"
2. **WAIT** - Show response synthesizing across entire corpus
3. **POINT OUT** multiple citations from different sources
4. (Optional) Click the **Notes icon** (ğŸ“š) to show accumulated knowledge

#### **SAY:**
> "This is reasoning across entire knowledge basesâ€”hundreds of papers, weeks of lecturesâ€”in one pass. Combined with Graphon's trillion-token persistent memory, there's virtually no limit."

**KEY POINTS**:
- âœ… 1M token context
- âœ… Entire knowledge bases
- âœ… Perfect for legal, research, finance applications

---

### **SECTION 5: GENERATIVE UI & VISUALIZATION** â±ï¸ 1.5 minutes

#### **SAY:**
> "Gemini 3 doesn't just answerâ€”it generates complete interactive visualizations. Watch this."

#### **DO:**
1. Type in input:
   > "Visualize the architecture of a transformer neural network based on the papers I uploaded"
2. Press **Enter**
3. **WAIT** - Response generates with diagram
4. **POINT OUT** the diagram as it appears
5. **CLICK** on the diagram
6. Full-screen SVG modal opens
7. **SAY**: "Full-screen inspection"
8. Close modal (click X or press Escape)
9. (Optional) Ask for another visualization:
   > "Create a flowchart showing the research methodology"

#### **SAY (while generating):**
> "Gemini 3 generates complete interactive visualizations, not just text responses. This is generative UI in actionâ€”the model is creating the entire interface."

#### **SAY (when modal opens):**
> "You can click any visualization to inspect it in full-screen mode. Perfect for understanding complex systems."

**KEY POINTS**:
- âœ… Generative UI (use this exact phrase)
- âœ… Interactive visualizations
- âœ… Not just textâ€”complete interfaces

---

### **SECTION 6: RESEARCH AGENT** â±ï¸ 1 minute

#### **SAY:**
> "For complex questions, we deploy a research agent that orchestrates multiple tools: web search, knowledge graph queries, and synthesis."

#### **DO:**
1. Press **Cmd+K** to open Command Palette
2. Click **"Toggle Deep Research"** (microscope icon)
3. Verify: Shows "ACTIVE" in green
4. Press **Escape**
5. Type question:
   > "What's the latest research on attention mechanisms from 2024, and how does it compare to what's in my uploaded papers?"
6. Press **Enter**
7. **WATCH** and **NARRATE** as steps appear:
   - "Planning research steps..."
   - "Step 1: Searching web for latest research..."
   - "Step 2: Querying knowledge graph for your papers..."
   - "Step 3: Synthesizing findings..."
8. **WAIT** for final comprehensive answer

#### **SAY (while agent works):**
> "Our research agent orchestrates multiple tools: web search for current information, Graphon for your knowledge base, and Gemini 3 for synthesis. This is multi-agent orchestration."

#### **SAY (when done):**
> "Notice how it combined external web search with your internal knowledge base to give you a comprehensive answer. This is what multi-agent reasoning enables."

**KEY POINTS**:
- âœ… Multi-agent orchestration
- âœ… Tool integration
- âœ… Multi-step reasoning

---

### **SECTION 7: COMPARE MODE** â±ï¸ 30 seconds

#### **SAY:**
> "Sometimes you want to see different perspectives. Compare Mode triggers two parallel inference streams."

#### **DO:**
1. Click the **COMPARE button** (toggle in input area, shows two arrows)
2. Verify: Button highlights in cyan
3. Type:
   > "Explain quantum computing"
4. Press **Enter**
5. **WAIT** - Two responses generate side-by-side
6. **POINT OUT** the split-screen view

#### **SAY:**
> "Perfect for comparing code implementations, checking for hallucinations, or seeing creative variations. Two complete responses in parallel."

**KEY POINTS**:
- âœ… Dual-stream generation
- âœ… Split-screen comparison
- âœ… Use cases

---

### **SECTION 8: SOCRATIC MODE** â±ï¸ 30 seconds

#### **SAY:**
> "And here's something unique: Socratic Mode. Instead of giving answers, the AI guides you through questioning."

#### **DO:**
1. Click the **Mode Toggle** button (top bar, shows "DIRECT")
2. Button changes to "SOCRATIC"
3. Type:
   > "How does backpropagation work in neural networks?"
4. Press **Enter**
5. **WAIT** - AI asks questions back instead of answering

#### **SAY:**
> "This forces you to reason, not just consume. Perfect for deep learning and ensuring true understanding."

**KEY POINTS**:
- âœ… Unique teaching feature
- âœ… Guided discovery
- âœ… Deep learning

---

### **SECTION 9: CLOSING** â±ï¸ 30 seconds

#### **SAY:**
> "We've combined:
> - **Gemini 3's native multimodality** for cross-modal reasoning
> - **1M token context** for entire knowledge bases
> - **Generative UI** for interactive visualizations
> - **Graphon AI** for persistent, trillion-token knowledge graphs
> - **Research agents** for multi-step reasoning
> 
> This couldn't have existed six months ago. It's the future of research and education."

#### **DO:**
1. Show a final impressive view:
   - Knowledge graph active (purple indicator)
   - Multiple visualizations visible
   - Citations showing
2. **POINT** to different elements as you list them

#### **SAY (final line):**
> "Thank you. Questions?"

**KEY POINTS**:
- âœ… List all technologies
- âœ… Emphasize: "Couldn't exist 6 months ago"
- âœ… Impact statement

---

## ğŸ¯ **KEY PHRASES TO MEMORIZE**

### **Upload Section:**
- "Graphon is building a relationship graph across all modalities in real-time"
- "Trillion-token persistent memory that never runs out"

### **Query Section:**
- "Native multimodalityâ€”not RAG stitching"
- "Exact citations with timestamps and page numbers"

### **Visualization Section:**
- "Generative UIâ€”the model creates the entire interface"
- "Complete interactive visualizations, not just text"

### **Research Agent Section:**
- "Multi-agent orchestration"
- "Tool integration and synthesis"

### **Closing:**
- "This couldn't have existed six months ago"

---

## âš ï¸ **TROUBLESHOOTING DURING DEMO**

### **If Graphon Upload is Slow:**
**SAY**: "The first upload takes 30-60 seconds as Graphon builds the relationship graph. Once it's done, queries are instant."

### **If Visualization Doesn't Generate:**
**SAY**: "Let me try a different prompt..." (then try: "Create a diagram of...")

### **If API Error:**
**SAY**: "The architecture supports thisâ€”this is just a connectivity issue. In production, this would work seamlessly."

### **If Backend Disconnects:**
**SAY**: "Let me show you the local knowledge base mode..." (switch to notes/syllabus demo)

---

## ğŸ“Š **TIMING CHECKLIST**

```
0:00 - Opening (30s) âœ…
0:30 - Upload (1m) âœ…
1:30 - Query (1.5m) âœ…
3:00 - Long-context (1m) âœ…
4:00 - Visualization (1.5m) âœ…
5:30 - Research Agent (1m) âœ…
6:30 - Compare Mode (30s) âœ…
7:00 - Socratic Mode (30s) âœ…
7:30 - Closing (30s) âœ…
```

**TOTAL: ~7-8 minutes** (with buffer)

---

## ğŸ¤ **PRESENTATION TIPS**

### **Energy Level:**
- âœ… High energy, enthusiastic
- âœ… Confident tone
- âœ… Smooth transitions

### **Body Language:**
- âœ… Point to screen elements as you talk
- âœ… Make eye contact with judges
- âœ… Gesture to show relationships

### **Pacing:**
- âœ… Don't rushâ€”7 minutes is plenty
- âœ… Pause after key demonstrations
- âœ… Let visuals speak for themselves

### **Focus:**
- âœ… Emphasize hackathon keywords
- âœ… Connect features to themes
- âœ… Show, don't just tell

---

## ğŸ† **PRIZE CONNECTIONS**

### **When to Mention Graphon Track ($1,000):**
- âœ… During upload section
- âœ… When showing citations
- âœ… Mention: "Best multimodal search tool"

### **When to Mention Grand Prize ($50K):**
- âœ… Opening: "Complete platform"
- âœ… Closing: "All Gemini 3 capabilities"
- âœ… Mention: "Couldn't exist 6 months ago"

### **When to Mention Antigravity ($25K):**
- âœ… Research agent section
- âœ… Mention: "Multi-agent orchestration"
- âœ… Mention: "Tool integration"

---

## âœ… **PRE-DEMO FINAL CHECK**

### **5 Minutes Before:**
- [ ] Both servers running
- [ ] Test one query works
- [ ] Test Graphon upload works
- [ ] Test visualization generates
- [ ] Demo files ready
- [ ] Browser tab ready (no other tabs)
- [ ] Screen resolution good
- [ ] Microphone/audio working (if needed)

### **1 Minute Before:**
- [ ] Take a deep breath
- [ ] Review opening line
- [ ] Have backup plan ready
- [ ] Smile ğŸ˜Š

---

## ğŸ¬ **FINAL REMINDERS**

1. **Start with the problem** - Information overload
2. **Show Graphon FIRST** - It's your strongest feature
3. **Use hackathon keywords** - Native multimodality, 1M token, generative UI
4. **Show, don't just tell** - Point to screen elements
5. **End with impact** - Future of research/education
6. **Be confident** - You built something amazing!

---

**YOU'VE GOT THIS! ğŸš€**

Good luck with your demo! Remember: You're showcasing technology that didn't exist 6 months ago. That's impressive.
